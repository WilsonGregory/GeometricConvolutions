% Copyright 2022 the authors

% To-Do list:
% -----------
% - What are our GOALS!!?
%   -- one option: this formulation can express all equivariant differential operators discretized to a cubical d-torus. 
%   -- another option: can we produce a characterization of all equivariant functions (maybe based on invariants or these filters) that don't require the use of non-linearities.
%   -- another option: emulate cosmological simulations.
% - change N to N^3 in the appropriate locations; already done??
% - Understand what functions can and cannot be expressed on this model 
% - Extend it to non-linear maps (eg, by multiplications and contractions)
% - Should we be representing images in fourier space? Ask Leslie!
% - Understand the literature
%   -- Discretized vector calculus and differential geometry.
%   -- Group-equivariant CNNs; what's been done?
%   -- Understand how this formulation compares with steerable CNNs
%   -- What is known about what functions CNNs can represent? all linear translation equivariant functions can be written as convvolutions if the filters are as large as the image (simple observation)
% what about non-linear? is there a polynomial basis here? 
%  -- Define "local" functions, is there a characterization of functions that can be written as convolutions with small filters?

% -- if the size of the filter is large enough, are we universal???? or can we express all polynomial functions? is there a version of Stone-Weierstrass for this?
% --- add the identity and levi civita in the possible output tensors
% --- define reordering of the indices
% --- we don't need to be in the torus and we don't need to be square (or cubical)
% --- generalize convolution to rectangular non-toroidal images


%-- is it the same to do convolutions + products + convolutions + products 

%-- a counting argument may tell us that all the linear functions that are equivariant  (Molien's formula)

%-- BEN: compute Molien's formula for this (translations semidirect D_4) action
% 1/|G|\sum_{g\in G} det(I-\phi(g)t)^{-1}

% write the levi civita contraction
% write the procedure of which we make all linear functions from k tensor images to k' tensor images of the same size. 
% do we include a pooling move?

% how do we enumerate all tensor functions

% 1, 6, 22, 49, 87 d=3, n=1,3,7,9 (11N^2-24N+21)/8

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper]{geometry} % because Overleaf is weird.
\PassOptionsToPackage{hyphens}{url}
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}

% stuff for framing figures
\usepackage[framemethod=tikz]{mdframed}
\usetikzlibrary{shadows}
\definecolor{captiongray}{HTML}{555555}
\mdfsetup{%
  innertopmargin=2ex,
  innerbottommargin=1.8ex,
  linecolor=captiongray,
  linewidth=0.5pt,
  roundcorner=5pt,
  shadow=true,
  shadowcolor=black!05,
  shadowsize=4pt
}

% theorem, definition and so on
\theoremstyle{plain}
\newtheorem{definition}{Definition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{theorem}{Theorem}

% math macros
% HOGG SAY: I DON'T KNOW HOW TO USE AMSMATH \choose AND I'M ON A PLANE. FEEL FREE TO FIX THIS.
\renewcommand{\choose}[2]{\begin{pmatrix}{#1}\\{#2}\end{pmatrix}}

% text macros
\newcommand{\sectionname}{Section}
\newcommand{\secref}[1]{\sectionname~\ref{#1}}

% typesetting adjustments
\makeatletter
\renewcommand\section{\@startsection {section}{1}{\z@}%
  {-3.25ex \@plus -1ex \@minus -.2ex}%
  {1.5ex \@plus .2ex}%
  {\raggedright\normalfont\large\bfseries}}
\makeatother
\setlength{\textwidth}{5.00in}
\setlength{\oddsidemargin}{3.25in}
\addtolength{\oddsidemargin}{-0.5\textwidth}
\setlength{\textheight}{9.40in}
\setlength{\topmargin}{-0.50in}
\setlength{\footskip}{0.25in}
\pagestyle{myheadings}
\markboth{foo}{Hogg, Blum-Smith, \& Villar / Geometric filters for geometric images}
\newcommand{\figurerule}{\rule[1ex]{\textwidth}{0.2pt}}
\linespread{1.08}
\frenchspacing\sloppy\sloppypar\raggedbottom

\title{\bfseries%
Convolution-based functions
on tensor fields and tensor images}
\author{David W. Hogg, Ben Blum-Smith, and Soledad Villar}
\date{}

\begin{document}

\maketitle\thispagestyle{empty}

\paragraph{Abstract:}
Convolutional neural networks are the main tools for machine learning with images at the present day.
These methods effectively assume that the input images are scalar images (an intensity or brightness in each pixel, possibly in three or more channels), and they enforce a kind of locality and translation-equivariance.
In natural-science domains, image-like data sets can have a vector (velocity, say), or tensor (polarization, say), or pseudovector (magnetic field, say), or any combination in each pixel.
We provide a construction for general convolution-based functions of images or lattices
containing scalars, vectors, and tensors, producing scalar, vector, and tensor outputs.
Our formulation, which combines convolutions with products, tensor contractions, and tensor index permutations, does not require point-wise activation functions (RELU, say) but can universally approximate any suitably local analytic scalar, vector, or tensor functions on scalar, vector, or tensor images.
Our method permits, with a very simple adjustment, restriction to function spaces that are exactly equivariant to translations, discrete rotations, and reflections.

\section{Introduction}

Contemporary natural science teems with data sets that are images or lattices or grids of geometric objects.
Rectangular images or lattices of density (a scalar field), temperature (also a scalar field), velocity (a vector field), magnetic field (a pseudo-vector field), and polarization (a 2-tensor field) are regularly taken with detectors or made in simulations.
That is, there is plenty of data consisting of geometric objects on regular grids.
These kinds of images (grids of scalars, vectors, pseudo-vectors, and tensors, for example) are \emph{geometric} in the sense that they contain (at every pixel) objects are defined by their transformation properties under rotation and reflection.
For example, scalars are defined to be quantities in $\mathbb R$ that are invariant with respect to all rotations and reflections; vectors are defined (in $d$-dimensional space) to be quantities in $\mathbb R^d$ that transform under rotation or reflection by multiplication by the orthogonal $d\times d$ matrix representing the rotation or reflection; and so on.

For a concrete example close to one of our hearts (DWH's), contemporary cosmological n-body simulations comprise the most computationally expensive part of most contemporary cosmological projects, missions, and experiments (CITE THINGS).
For this reason, accurate, fast emulators might be of enormous economic and scientific value.
There are now quite a few successful projects implementing such emulators (CITE THINGS).
Importantly, the initial conditions for these simulations are tiny displacements (vectors) and velocities (vectors) relative to a cubical lattice on a 3-torus.
That is, the initial conditions can be described as a 3-dimensional image of vectors on a regular grid.
For another concrete example, astronomical imaging data taken of the interstellar medium can be represented as rectangular lattices of measurements of temperature (scalar), optical depth (scalar), and induced polarization of starlight (a 2-tensor).
Here the data can be described as a 2-dimensional image of scalars and tensors on a regular grid.

The application of machine-learning techniques to these problems can be dangerous:
Machine-learning models have enormous freedom; the laws of physics are constrained to obey a large set of extremely strict symmetries, including rotation, reflection, translation, permutation, and boost equivariances, plus constraints such as units and dimensions symmetry, unitarity, and myriad conservation laws.
If we want to operate safely on these problems, we want these symmetries built in from the start, and exactly.

HOGG: Differences between continuous and discrete symmetries.

HOGG: Summarize here how we are going to proceed, in human language.

\paragraph{Related work:}

\paragraph{Our contribution:}

\section{Geometric objects and geometric images}\label{sec:geometric}

In this \sectionname{} we define the geometric objects and geometric images that we use to generalize classical images in scientific contexts.
The main point is that the channels of geometric images---which will be like the components of vectors and tensors---are not independent.
There is an underlying action by the orthogonal group that rules what are the allowed objects and transformations in this space.

We first fix $d$, the dimension of the space (typically 2 or 3 but it could be arbitrary). The geometric objects are vectors and tensors. 
The orthogonal group $O(d)$ can act on $v \in \mathbb R^d$ in the following ways:
\begin{align} \label{eq.action}
    g\cdot v = \det(M(g))^p M(g) v 
\end{align}
where $g\in O(d)$, $M(g)\in \mathbb R^{d\times d}$ is the standard matrix representation of $g$, and $p\in\{0,1\}$ is the parity of the action. If $p=0$ we obtain the standard $O(d)$ action on $\mathbb R^d$ \emph{vectors}. If $p=1$ we obtain what in physics is commonly known as the action of $O(d)$ on \emph{pseudovectors}. In this context the objects are defined by the actions that they carry.

\begin{definition}[$k$-$p$-tensors] \label{def.tensors}
We say that $v\in \mathbb R^d$ is a 1-$p$-tensor if $O(d)$ acts on $v$ via the action \eqref{eq.action}. 
If $v_i$ is a 1-$p_i$-tensor then $T:=v_{1}\otimes\ldots \otimes v_k$ is a rank-1 $k$-$p$-tensor, where $p=\sum_{i=1}^k p_i \mod 2$ and the action of $O(d)$ is defined as
\begin{align}
    g\cdot (v_{1}\otimes\ldots \otimes v_k) = (g\cdot v_1)\otimes \ldots \otimes (g\cdot v_k)
\end{align}
Higher rank $k$-$p$-tensors are defined as linear combinations of rank-1  $k$-$p$-tensors where the action of $O(d)$ is extended linearly.
\end{definition}

We note that in physics the 1-0-tensors are known as \emph{vectors}, the 1-1-tensors are \emph{pseudovectors}, the 0-0-tensors are \emph{scalars}, 0-1-tensors are \emph{pseudoscalars}, the $k$-1-tensors with $k\geq 2$ are \emph{pseudotensors}, and finally the $k$-0-tensors with $k\geq 2$ are known as \emph{tensors}.


\begin{definition}[summation notation]
In this notation, tensor products are written in component form, and repeated indices are summed over.
In this notation, the product of two 2-tensors (represented as two $d\times d$ matrices $A$ and $B$) is written as
\begin{equation}
    [A\, B]_{i,j} = [A]_{i,k}\,[B]_{k,j} ~,
\end{equation}
where $[A]_{i,k}$ (for example) is the $i,k$ element of matrix $A$, the repeated index $k$ is implicitly summed over the integers from 1 to $d$.
This notation works for tensor expressions of any order, provided that every index appears either exactly once (so it isn't summed over; in the above expression $i, j$ appear exactly once) or exactly twice (so it is summed over; in the above expression $k$ appears exactly twice). 
\end{definition}

One consequence of the above definitions is that 
$T\in (\mathbb R^d)^{\otimes k}$ is a $k$-$p$-tensor if and only if (in summation notation)
\begin{align}
    [g\cdot T]_{i_1,\ldots, i_k} = \det(M(g))^p\, [T]_{j_1,\ldots,j_k}\,[M(g)]_{i_1,j_1}\cdots[M(g)]_{i_k,j_k}
\end{align} for all $g\in O(d)$, where $[T]_{i_1, \ldots ,i_k} \in \mathbb R$ is a component of $T$, $[M(g)]_{i,j}\in\mathbb R$ is the $i,j$ element of the matrix representation of $g$, and all the $i_m$ and $j_m$ are indices in the range $1,\ldots,d$.
For example, a 2-0-tensor is defined by its transformation property
$[g\cdot T]_{i,j} = [T]_{k,\ell}\,[M(g)]_{k,i}\,[M(g)]_{\ell,j}$,
which, in normal matrix notation, is written as
$g\cdot T = M(g)\,T\,M(g)^\top$.

We consider an image $A$ in with $N$ equally spaced pixels in each dimension ($N^d$ pixels total). Each pixel contains a $k$-$p$-tensor ($k$ and $p$ are the same for each pixel). Sometimes we will consider the image to be in a $d$-torus. 
Let $\mathcal T_{d,k,p}$ the set of $k$-$p$-tensors in $\mathbb R^d$. We define the geometric images as follows.
\begin{definition}[geometric image]
A geometric image is a function $A:[N]^d \to \mathcal T_{d,k,p}$, where $[N]=\{0,1,\ldots, N-1\}$. We will also consider $k$-tensor images on the $d$-torus, where $[N]^d$ is given the algebraic structure of $(\mathbb Z / N\mathbb Z)^d$.
\end{definition}

\section{Generalized convolutions on geometric images}\label{sec:convolution}

In this \sectionname{} we generalize the notion of convolution to take geometric images as inputs and return geometric images as outputs.
The idea is that the (presumably large) geometric image (containing $k$-$p$-tensors) is convolved with a (presumably small) geometric filter (also a geometric image, but containing $k'$-$p'$-tensors) to produce an output image that contains $(k+k')$-$(p+p')$-tensors, where each pixel is a sum of outer products.
These outer products can be contracted down to lower-$k$ tensors using Kronecker or Levi-Civita contractions.

\begin{definition}[outer products of tensors]
If $a$ is a $k$-$p$-tensor and $b$ is a $k'$-$p'$-tensor, then the outer product $a\otimes b$ is a $(k+k')$-$(p+p')$-tensor with $[a\otimes b]_{i_1,\ldots,i_{k+k'}} = [a]_{i_1,\ldots,i_k}\,[b]_{i_{k+1},\ldots,i_{k+k'}}$
\end{definition}

If the next definition is going to use ``the torus'' then we should say a few words about that up-front, or above in the image definition.

\begin{definition}[geometric convolution]
Let $A$ be a $k$-$p$-tensor image on the $d$-torus with side length $N$.
Let $C$ be a $k'$-$p'$-tensor image (filter) on $[-m, m]^d$ where $2m+1<N$.
The geometric convolution $A\ast C$ is a $(k+k')$-$(p+p')$-tensor image such that
\begin{equation}
    (A\ast C)(\bar\i) = \sum_{\bar a\in[-m, m]^d} A(\bar\i - \bar a)\otimes C(\bar a) ~,
\end{equation}
where $\bar\i - \bar a$ is the translation of $\bar\i$ by $\bar a$ on the $d$-torus pixel grid $(\mathbb Z / N\mathbb Z)^d$.
\end{definition}
This definition is on the torus. If you are not in the torus just pad with zero tensors of the corresponding order and parity. 

Convolutions can be followed by Kronecker contractions, Levi-Civita contractions and index permutations.
The Kronecker delta is the object with two indices such that it has the value $+1$ when the two indices have the same value, and $0$ otherwise.
Contraction of a $k$-$p$-tensor (with $k\geq 2$) with one application of the Kronecker delta returns a $(k-2)$-$p$-tensor.
Contraction of a $k$-$p$-tensor (with $k\geq 2$) with one application of the Kronecker delta returns a $(k-2)$-$p$-pseudotensor, because the contraction sums out one pair of indices.
The Levi-Civita symbol in $d\geq 2$ spatial dimensions is the object with $d$ indices (where $d$ is the spatial dimension) such that if the $d$ indices are not repeated and in an even-permutation order the value is $+1$ and if the $d$ indices are not repeated and in an odd-permutation order the value is $-1$, and it has the value $0$ in all other cases.
Contraction of a $k$-$p$-tensor (with $k\geq (d-1)$) with one application of the Levi-Civita symbol returns a $(k-d+2)$-$(p+1)$-pseudotensor, because the contraction sums over $d$ pairs of indices, and it changes the parity from even to odd or odd to even.
For one example of these contractions, the 2-tensor formed by the outer product of 1-tensors $a$ and $b$ can be contracted with the Kronecker delta to give the standard dot product $a^\top b = [a]_i\,[b]_j\,\delta_{ij}$, which is a 0-tensor or scalar.
For another example, the same 2-tensor can be contracted with the Levi-Civita symbol to give the standard cross product
$[a\times b]_k = [a]_i\,[b]_j\,\epsilon_{ijk}$, which is a pseudovector.

\begin{definition}[Kronecker contractions of tensors]
Let $a$ be a $k$-$p$-tensor in $\mathbb R^d$ with $k\geq 2$ such that $[a]_{i_1,\ldots i_k}\in \mathbb R$ for all $i_s = 1,\ldots, d$, and $s=1,\ldots, k$. We define the $(\alpha,\beta)$ Kronecker contraction of $a$ to be the $k-2$ tensor resulting from summing jointly over the indices $i_\alpha$ and $i_\beta$ where $\alpha < \beta \in\{1,\ldots, k\}$. In Einstein summation notation it would be $a_{i_1, \ldots, x, \ldots , x, \ldots, i_k}$ where the $x$s are placed in the positions $\alpha$ and $\beta$:
\begin{equation}
[a^{(\alpha,\beta)}]_{i_1,\ldots, i_{\alpha-1}, i_{\alpha+1}, \ldots, i_{\beta-1}, i_{\beta+1} , \ldots, i_k} := \sum_{x=1}^d [a]_{i_1, \ldots, i_{\alpha-1}, x, i_{\alpha+1}, \ldots, i_{\beta-1}, x, i_{\beta+1}. \ldots, i_k}
\end{equation}
\end{definition}
There are $\choose{k}{2}$ different index choices for this contraction, corresponding to all the possible index pairs among $k$ original indices.

\begin{definition}[Levi-Civita contractions of tensors]
Let $a$ be a $k$-$p$-tensor in $\mathbb R^d$ with $k\geq d-1$ such that $[a]_{i_1,\ldots i_k}\in \mathbb R$ for all $i_s = 1,\ldots, d$, and $s=1,\ldots, k$. We define the Levi-Civita contraction of $a$ to be the $(k-d+2)$-$(p+1)$-tensor resulting from taking the outer product of 
$a$ with the $d$-index Levi-Civita symbol $\epsilon_{i_1,i_2,\ldots,i_d}$ and then performing $d-1$ Kronecker contractions in which the first index is chosen from the (remaining) indices of $a$ and the second index is chosen from the (remaining) indices of the Levi-Civita symbol.
\end{definition}
There are $(d-1)!\,\choose{k}{d-1}$ different index choices for this contraction corresponding to an ordered list of $d-1$ indices chosen from among $k$ choices.

\begin{definition}[permutations of tensor indices]
Given a $k$-$p$-tensor $a\in (\mathbb R^d)^{\otimes k}$ and $\sigma\in S_k$ a permutation. We consider $a^\sigma$ the result of permuting the indices of $a$ according to $\sigma$:
\begin{equation}
    [a^\sigma]_{i_1, \ldots, i_k} := [a]_{\sigma^{-1}(i_1), \ldots, \sigma^{-1}(i_k)}
\end{equation}
\end{definition}

Define POOLING operation?

The following is not correct -- doesn't include permutations or pooling, and doesn't get the combinatorics right.

Now imagine that we want to find \emph{all} linear equivariant functions that go from an input $k$-tensor image of parity $p$ (SOLE: DO WE USE THIS CONCEPT OF PARITY PREVIOUSLY? IF NOT, IT SHOULD BE DEFINED. IT MIGHT SIMPLIFY OUR SHIT.) to an output $k'$-tensor image of parity $p'$, such that the input and output images are of the same size (both $N^d$).
Here are the linear operations (linear in the input image) that are permitted):
\begin{itemize}
\item Choose an integer $\ell$ in the range $0\leq\ell\leq k$ such that $(k'-k+2\,\ell)>0$.
  Convolve the input $k$-tensor image with a $(k'-k+2\,\ell)$-tensor invariant filter of parity $p\,p'$.
  Contract the resulting $(k'+2\,\ell)$-tensor image (of parity $p'$) with the Kronecker delta $\ell$ times, always choosing at least one index from the indices of the input image.
  There are $$\prod_{i=1}^{\ell}\frac{1}{\ell!}\,\left[\choose{k'+2\,i}{2}-\choose{k'-k+2\,i}{2}\right]$$ possible choices for these $\ell$ Kronecker contractions.
\item Choose an integer $\ell$ in the range $0\leq\ell\leq k-d+1$ such that $(k'-k+d-2+2\,\ell)>0$.
  Convolve the input $k$-tensor image with a $(k'-k+d-2+2\,\ell)$-tensor invariant filter of parity $-p\,p'$.
  Contract the resulting $(k'+d-2+2\,\ell)$-tensor image (of parity $-p'$) with the Levi--Civita symbol.
  There are $$\left[\choose{k'+d-2+2\,\ell}{d-1}-\choose{k'-k+d-2+2\,\ell}{d-1}\right]$$ choices for this Levi--Civita contraction.
  Then contract the resulting $(k'+2\,\ell)$-tensor image (of parity $p'$) with the Kronecker delta $\ell$ times, always choosing one index from the indices of either the input image or of the Levi--Civita symbol.
  There are $$\prod_{i=1}^{\ell}\frac{1}{\ell!}\,\left[\choose{k'+2\,i}{2}-\choose{k'-k+2\,i}{2}\right]$$ possible choices for these $\ell$ Kronecker contractions. (SOLE: IS THIS CONSISTENT WITH LANGUAGE ABOVE?)
\end{itemize}

other things that may be useful: eigenvalues

is it obvious how this contractions and permutations apply to the entire image? the same permutation, contraction should be applied to all pixels of the image. 

\section{Higher-order functions of geometric images}\label{sec:nonlinear}

The convolution and contraction operations above defines a large class of linear functions from geometric images to geometric images. Non-linear functions can be constructed as products.

Algorithm: \newline
Construct all linear functions using the constructions from previous section. \newline
Construct all pixel-wise products across them. \newline
All linear functions of all pixel-wise products. \newline
That gives everything to second order. \newline

\begin{definition}[Products of geometric images]
Given an $N^d$-pixel $k_A$-tensor image $A$ and an $N^d$-pixel $k_B$-tensor image $B$, we can define a product $A\otimes B$, which is an $N^d$-pixel $(k_A+k_B)$-tensor image such that $(A\otimes B)(\bar\i)=A(\bar\i)\otimes B(\bar\i)$. These products can be contracted by the contraction rules above.
\end{definition}

\begin{definition}[Polynomial local geometric operator of degree $\ell$]
Given a $k$-tensor image $A$ and a set of $\ell$ $k'_j$-tensor geometric filters $C_j$, a geometric image can be formed by the degree-$\ell$ image product
\begin{equation}
    (A\ast C_1) \otimes (A\ast C_2) \otimes \cdots \otimes (A\ast C_\ell) ~. \label{monomial}
\end{equation}
This product can be subsequently contracted $r$ times to deliver a $K$-tensor geometric image with
\begin{equation}
    K = \ell\,k - 2\,r + \sum_{j=1}^\ell k'_j ~.
\end{equation}
A polynomial local geometric operator is a function that takes a $k$-tensor image $A$ and outputs the sum of $K$-tensor images constructed as contractions to order $K$ of tensor images from \eqref{monomial} with filters chosen by the user. 
\end{definition}

The geometric convolutions of \secref{sec:convolution} are local linear operators that take as input geometric images, and produce as output geometric images.
They are local in the sense that, at every pixel $\bar\i$ of the output image $(A\ast C)$, the convolution only makes use of the pixels of $A$ within a small neighborhood of of $\bar\i$.
Because tensors can be multiplied together using the outer product, and contracted according to the tensor contractions defined in \secref{sec:convolution}, it is possible, by multiplication, to construct a family of \emph{nonlinear} local geometric operators that take as input geometric images and produce as output geometric images.
This family of nonlinear local operators will be equivariant to the group $G_d$ when the linear local operators of which they are composed are equivariant.

Assembling the parts into a general, linear function.



With these definitions in hand, we can make geometric filters, take outer products, and contract to make a combinatorically large set of nonlinear geometric convolution operators on images made up of geometric objects.
But before we go nonlinear, we will go equivariant.

Assemble all the parts into an algorithm for making all possible monomials.

\section{Equivariant functions of geometric images}\label{sec:equivariant}

Define equivariance.

Prove translation invariance.

Prove thing about invariant filters.

We will now create linear convolution operators that are group-equivariant, for groups of reflections, rotations, and translations (on the cubical torus).
We will construct these equivariant convolution operators by an averaging over group operators, followed by a matrix factorization (singular-value decomposition).

\begin{definition}[Group $B_d$ of symmetries of a $d$-hypercube]
We denote by $B_d$ the group of rotation and reflection symmetries of the $d$-dimensional hypercube.
\end{definition}

Notes about the group $B_d$: In $d=2$ there are $8$ group elements, in $d=3$ there are $48$ group elements (and in $d=4$ there are 384). Because the groups $B_d$ are subgroups of $O(d)$, all determinants of the matrix representations of the group elements are either $+1$ or $-1$, and the matrix representation $M(g^{-1})$ of the inverse $g^{-1}$ of group element $g$ is the transpose of the matrix representation $M(g)$ of group element $g$.

\begin{definition}[Action of $B_d$ on $k$-tensors and $k$-pseudotensors]
\begin{align}
\mbox{$k$-tensor:} ~~ g\cdot(v_1 \otimes \ldots \otimes v_k) &= (g\cdot v_1)\otimes\ldots\otimes(g\cdot v_k) \\
\mbox{$k$-pseudotensor:} ~~ g\cdot(v_1 \otimes \ldots \otimes v_k) &= \det(g)\,(g\cdot v_1)\otimes\ldots\otimes(g\cdot v_k) ~,
\end{align}
And extend linearly
\end{definition}

This may seem familiar: The $k$-tensor and $k$-pseudotensor were defined by how they transform under the orthogonal group $O(d)$, and our group $B_d$ is a subgroup of the orthogonal group.

\begin{definition}[Action of $B_d$ on $k$-tensor images]
Given a $k$-tensor image $A$ on the $d$-torus, and a group element $g\in B_d$, the action $g\cdot A$ produces a $k$-tensor image on the $d$-torus such that
\begin{equation}
    (g\cdot A)(\bar\i) = g\cdot A({g^{-1}\cdot \bar\i}) ~.
\end{equation}
SOmeThiNg about how $g^{-1}\cdot\bar\i$ works here, possibly preceded or followed by proofs. Because of what follows, this has to be defined BOTH for the $N^d$ boxels in the $d$-torus and on the $(2\,m+1)^d$ boxels of the filter (image patch).
\end{definition}

It might be a bit surprising that the group element $g^{-1}$ appears in the definition of the action of the group on images.
One way to think about it is that the pixels in the output (rotated, say) image are ``looked up'' or ``read out'' from the pixels in the original (unrotated) image.
The pixel locations in the original image are found by going back, or inverting the transformation.

\begin{definition}[The group $G_d$, and its action on $k$-tensor images]
THIS NEEDS TO BE WRITTEN: $G_d$ is the group generated by the elements of $B_d$ and the discrete translations on the $N^d$-pixel lattice on the $d$-torus.
\end{definition}

HOGG: Comment on the fact that $G_d$ is a subgroup of the Euclidean group, but that we care about the Euclidean group because we are imagining that the discrete images are images of a continuous world!!

\begin{definition}[Equivariant convolution]
We say that a convolution filter $C$ produces convolutions that are equivariant with respect to the group $G_d$ if for any $k$-tensor image $A$ on the $d$-torus, and any group element $g\in G_d$, we have
\begin{equation}
    g\cdot (A\ast C) = (g\cdot A)\ast C ~,
\end{equation}
where the group actions are defined above.
\end{definition}

The intuition here is that the filter $C$ represents some physical law, and the image $A$ represents the initial conditions or state of the physical system on which the law is acting.
The physical law (represented by $C$) is invariant to translations, rotations, and parity, while the \emph{output} of the physical law (represented by $A\ast C$) is \emph{equivariant} to translations, rotations, and parity acting on the state $A$.

The following theorem generalizes the Cohen and Welling paper:

\begin{theorem}
A $k'$-tensor (or $k'$-pseudotensor) convolution filter $C$ produces convolutions that are equivariant with respect to the big group $G_d$ if $C$ is invariant under the small group $B_d$.
\end{theorem}

\begin{proof}
By Definition \ref{} we have
\begin{eqnarray}
(g\cdot (A* C) )(\bar \i) &=& g\cdot( (A*C)(g^{-1} \cdot \bar \i)) \\
&=& g \cdot \left (\sum_{a\in [-m,m]^d} A(g^{-1}\cdot \bar \i - a) \otimes C(a)\right)\\
&=& \sum_{a\in [-m,m]^d} g \cdot A(g^{-1} \cdot \bar \i -a ) \otimes g \cdot C(a).
\end{eqnarray}
By Definition \ref{} we have
\begin{eqnarray}
((g\cdot A)* C) (\bar \i) &=& \sum_{a\in [-m,m]^d} g\cdot A(g^{-1} \cdot (\bar \i - a) ) C(a) \\
&=& \sum_{a'\in [-m,m]^d} g \cdot A(g^{-1}\cdot \bar \i -a') \otimes C(g\cdot a'),
\end{eqnarray}
where $a'=g\cdot a$. Therefore the convolution is equivariant if $g\cdot C(g^{-1} \cdot a') = C(a')$ for all $g\in B_d$ and all $a' \in [-m,m]^d$. 
\end{proof}

\subsection{Construction of equivariant geometric convolution filters}
In the above we defined nonlinear equivariant geometric operators acting on $d$-dimensional images on the $d$-torus.
These, in turn, are constructed from products of the outputs of equivariant geometric convolution filters.
Here we show how to find all such filters by a group-averaging technique.

Briefly, the idea is the following:
We start with a set of independent $k'$-tensor (or $k'$-pseudotensor) convolution filters that span the space of all possible geometric convolution filters at side length $M$ ($M$ odd) and dimension $d$.
There are typically $M^d\,d^{k'}$ independent filters in this set.
We average the filters over all group operators $g\in B_d$, where $B_d$ is the group of 90-degree rotations and reflections.
We perform a singular value decomposition of these group-averaged filters to find the $p$ independent orthogonal filters that span the space of equivariant $k'$-tensor (or $k'$-pseudotensor) convolution filters at side length $M$ in $d$ dimensions.

In detail, group averaging happens HOW? That is, what is the exact mathematical operation?

\section{Expressive power of geometric convolutions}\label{sec:universality}

Counting arguments.

Comments on locality.

We can express all linear translation equivariant functions (by just using convolutions assuming filters are same size of the image). 

We can cite that convolutions with small patches may be seen as low pass filters in some way. Reference?



\section{Implementation and numerical examples}\label{sec:examples}

How do we make the invariant filters?

How do we make the linear functions?

What do they look like?

Etc.

An example?

\begin{figure}[tp]
  \begin{mdframed}
  \color{captiongray}
  \begin{center}
\includegraphics[width=\textwidth]{notebooks/filter+0_2_3.png}\\
\includegraphics[width=\textwidth]{notebooks/filter+1_2_3.png}\\
\includegraphics[width=\textwidth]{notebooks/filter-1_2_3.png}
\includegraphics[width=\textwidth]{notebooks/filter+2_2_3.png}\\
\includegraphics[width=\textwidth]{notebooks/filter-2_2_3.png}
  \end{center}
\caption{All the filters for $d=2$, $m=1$. HOGG: notes!}
  \end{mdframed}
\end{figure}

\begin{figure}[tp]
  \begin{mdframed}
  \color{captiongray}
  \begin{center}
\includegraphics[width=\textwidth]{notebooks/filter+0_2_5.png}\\
\includegraphics[width=\textwidth]{notebooks/filter-0_2_5.png}\\
\includegraphics[width=\textwidth]{notebooks/filter+1_2_5.png}\\
\includegraphics[width=\textwidth]{notebooks/filter-1_2_5.png}
  \end{center}
\caption{All the filters for $d=2$, $m=2$. HOGG: notes!}
  \end{mdframed}
\end{figure}

Give a table saying how many filters there are for each $d$ and each $M$ at each $k$ and each parity.

Now show some fake-data geometric images and show the action of the $d=2$, $M=3$ filters on those images.

\begin{figure}
  \begin{mdframed}
  \color{captiongray}
  \begin{center}
    \includegraphics[width=\textwidth]{notebooks/monomials_1.png}
  \end{center}
    \caption{A scalar image $s$ and convolutions with four chosen filters.}
  \end{mdframed}
\end{figure}

Now show contractions of products of the above on those images.

\begin{figure}
  \begin{mdframed}
  \color{captiongray}
  \begin{center}
    \includegraphics[width=\textwidth]{notebooks/monomials_2.png}
  \end{center}
    \caption{All unconvolved second-degree monomials of $s$, given four chosen filters. For the 2-tensor outputs (order $k=2$), we traced them back to scalars.}
  \end{mdframed}
\end{figure}


\begin{itemize}
    \item find the invariant geometric filters
    \item geometric convolution operator
    \item pixel-wise outer product
    \item contraction operator, enumeration of all contractions
    \item output index permutation, enumeration of all permutations
    \item enumeration of all monomials up to a fix order and degree 
    \item remove repeated objects (SVD or just lstsq)?
    \item linear combinations 
    \item loss function
    \item optimization
\end{itemize}


\section{Discussion}\label{sec:discussion}

Hello world

One discussion point: you might be able to get back to a continuous symmetry group if you consider the image to be a VIEW of a large, continuous tensor field...?




Note that we never take eigenvalues. But these are also real scalars in some cases.

Note that pseudo-$k$-tensor filters only exist for certain triples of $(d, m, k)$.

\paragraph{Acknowlegements:}
This project made use of Python, numpy, matplotlib, and cmastro. 

\end{document}
